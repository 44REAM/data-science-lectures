{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Trustworthy Machine Learning Lecture at the University of T√ºbingen. \n",
    "Winter Semester 2023/2024. https://www.youtube.com/playlist?list=PL05umP7R6ij0FDHxle4CQLkzOfN-PAt7b ***\n",
    "- Mucs√°nyi B, Kirchhof M, Nguyen E, Rubinstein A, Oh SJ. Trustworthy Machine Learning 2023. ***\n",
    "- Guo C, Pleiss G, Sun Y, Weinberger KQ. On Calibration of Modern Neural Networks 2017. \n",
    "- Gawlikowski J, Tassi CRN, Ali M, Lee J, Humt M, Feng J, et al. A survey of uncertainty in deep neural networks. Artif Intell Rev 2023;56:1513‚Äì89. ***\n",
    "- H√ºllermeier E, Waegeman W. Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods. Mach Learn 2021;110:457‚Äì506. ***\n",
    "- Huang L, Ruan S, Xing Y, Feng M. A review of uncertainty quantification in medical image analysis: Probabilistic and non-probabilistic methods. Medical Image Analysis 2024;97:103223. ***\n",
    "- Angelopoulos AN, Kohli AP, Bates S, Jordan MI, Malik J, Alshaabi T, et al. Image-to-Image Regression with Distribution-Free Uncertainty Quantification and Applications in Imaging 2022.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustworthy AI 2: Uncertainty quantification\n",
    "\n",
    "Definition: Uncertainty Quantification (UQ) is the process of quantifying the level of confidence in a model‚Äôs predictions. It helps identify how sure or unsure the model is about its outputs.\n",
    "\n",
    "**Key Learning Objectives:**\n",
    "\n",
    "- Understand the role of UQ in building trustworthy AI.\n",
    "- Differentiate between types of uncertainty and their implications.\n",
    "- Explore techniques for measuring and handling uncertainty.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "## Why Uncertainty quantification\n",
    "\n",
    "![](images_uq/intro.png)\n",
    "\n",
    "- Human-in-the-Loop Decision Making\n",
    "\n",
    "    - Goal: Efficiently allocate human resources by only requiring human intervention when the model‚Äôs confidence is low.\n",
    "    - Mechanism: The model indicates uncertainty by saying, ‚ÄúI am not sure about the result.‚Äù In these cases, human experts can step in, assess the situation, and adjust the prediction if necessary.\n",
    "\n",
    "- Risk Management and Safety\n",
    "\n",
    "    - Goal: Minimize risks by ensuring the model only acts when confident.\n",
    "    - Mechanism: If the model is unsure, the system can halt processing or switch to a safe fallback state to avoid potential errors.\n",
    "    - Example: In a learning-based manufacturing robot for car assembly, if the robot is uncertain about its next action, it may pause or escalate to a safer state. Acting with uncertainty could lead to costly mistakes, including damaging the product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example use cases\n",
    "\n",
    "#### Example: Image search for disease\n",
    "\n",
    "![](images_uq/intro2.png)\n",
    "\n",
    "ref: https://blog.google/technology/health/ai-dermatology-preview-io-2021/\n",
    "\n",
    "An image search tool for skin disease detection that flags cases with high uncertainty for physician consultation.\n",
    "\n",
    "#### Example: Active learning\n",
    "\n",
    "![](images_uq/intro3.png)\n",
    "\n",
    "ref: Active Learning Literature Survey, Settles 2010\n",
    "\n",
    "In active learning, the model identifies instances it is uncertain about and prioritizes them for labeling, reducing the need for extensive labeled datasets.\n",
    "\n",
    "ref: https://www.gatsby.ucl.ac.uk/~balaji/balaji-uncertainty-talk-cifar-dlrl.pdf\n",
    "\n",
    "#### Object detection pipeline\n",
    "\n",
    "![](images_uq/intro4.png)\n",
    "\n",
    "- Object detection pipelines, such as in Faster R-CNN, use ‚Äúobjectness‚Äù scores to quantify confidence for each detected object.\n",
    "- The model proposes bounding boxes and provides a confidence score for each. This score helps prune unlikely boxes, improving both accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Uncertainty\n",
    "\n",
    "\n",
    "\n",
    "![](images_uq/uq_ex1.png)\n",
    "\n",
    "- f(x) is output\n",
    "- c(x) is probability that f(x) is correct (0 <= c(x) <= 1)\n",
    "\n",
    "![](images_uq/uq_ex2.png)\n",
    "\n",
    "face recognition\n",
    "\n",
    "- c(x) can be vector of confident\n",
    "\n",
    "![](images_uq/uq_ex3.png)\n",
    "\n",
    "A. N. Angelopoulos et al., ‚ÄúImage-to-image regression with distribution-free uncertainty quantification and applications in imaging,‚Äù PMLR, https://proceedings.mlr.press/v162/angelopoulos22a.html . \n",
    "\n",
    "- uncertainty also can quantify as a set of value (set of possible value of image in this case)\n",
    "\n",
    "![](images_uq/uq_ex4.png)\n",
    "\n",
    "- uncertainty also can quantify as a set of value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Classification Task\n",
    "\n",
    "- Given an input ùë•, the model produces an output ùëì(ùë•), which represents the predicted probability that ùë• belongs to the positive class. \n",
    "- Typically, a cut-off point of 0.5 is used.\n",
    "- If ùëì(ùë•)>0.5, classify as Positive (Class 1).\n",
    "\n",
    "In this case, we may define confidence $c(x)$ as $max(f(x), 1-f(x))$ and uncertainty as $1- c(x)$.\n",
    "\n",
    "![](images_uq/cx.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty evaluation\n",
    "\n",
    "## Calibration\n",
    "\n",
    "Calibration measures how well the predicted probabilities match the actual observed outcomes, indicating the accuracy of predictive uncertainty.\n",
    "\n",
    "- A well-calibrated model produces probabilities that reflect true likelihoods. For example, if a model predicts a probability of 0.7 for a positive outcome, that outcome should occur approximately 70% of the time.\n",
    "- Calibration helps assess the reliability of predictions, particularly useful in high-stakes applications where understanding prediction confidence is crucial.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating calibration\n",
    "\n",
    "### Qualitative\n",
    "\n",
    "![](images_uq/cal.png)\n",
    "\n",
    "Guo C, Pleiss G, Sun Y, Weinberger KQ. On Calibration of Modern Neural Networks 2017.\n",
    "\n",
    "\n",
    "- Left is well-calibrated model\n",
    "- Right is less well-calibrated model compare to the left\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined the perfect calibration as:\n",
    "\n",
    "$$P(\\hat{Y}=Y | \\hat{P}=p) = p$$\n",
    "\n",
    "- $\\hat{Y}$ is prediction class\n",
    "- $Y$ is real class\n",
    "- $\\hat{P}$ is prediction probability\n",
    "- $p$ is the prediction accuracy\n",
    "\n",
    "$$P(\\hat{Y}=1 | \\hat{P}=0.8) = 0.8$$\n",
    "\n",
    "This means that when the model predicts an instance as class 1 with an 80% confidence, it should be correct approximately 80% of the time. Calibration ensures that the model‚Äôs confidence aligns with real-world outcomes.\n",
    "\n",
    "### Expected calibration error (ECE)\n",
    "\n",
    "- ECE quantifies how far a model‚Äôs predicted probabilities are from perfect calibration.\n",
    "- ECE groups predictions into intervals (e.g., 0.1 to 0.2, 0.2 to 0.3, etc.), calculates the accuracy in each interval, and computes the average difference between accuracy and predicted probability.\n",
    "\n",
    "You have a set of all prediction $(\\hat{y}_j, \\hat{p}_j)$. We can calculate ECE as:\n",
    "\n",
    "1. Group the predictions into M bins (typically M = 10) based on the confidences estimates. Define bin $B_m$ to be the set of all predictions $(\\hat{y}_i, \\hat{p}_i)$ for which it holds that\n",
    "\n",
    "![](images_uq/cal_ece.png)\n",
    "\n",
    "2. Compute the accuracy and confidence of each bin $B_m$\n",
    "\n",
    "$$\\text{acc}(B_m) = \\frac{1}{|B_m|} \\sum_{i \\in B_m} \\mathcal{1}(\\hat{y_i}=y_i)$$\n",
    "$$\\text{conf}(B_m) = \\frac{1}{|B_m|} \\sum_{i \\in B_m} \\hat{p}_i$$\n",
    "\n",
    "3. Compute the ECE by taking the mean over the bins weighted by the number of samples in them.\n",
    "\n",
    "$$\\text{ECE} = \\frac{|B_m|}{n} \\sum_{m=1}^M |\\text{acc}(B_m) - \\text{conf}(B_m)|$$\n",
    "\n",
    "for $n$ is the number of all sample in the set\n",
    "\n",
    "The ECE measures the deviation of the model‚Äôs confidence predictions from the corresponding actual accuracies on a test set. It is a weighted average of bin-wise miscalibration.\n",
    "\n",
    "![](images_uq/ece.png)\n",
    "\n",
    "#### On Calibration of Modern Neural Networks\n",
    "\n",
    "Modern neural networks frequently demonstrate poor calibration compared to their predecessors. Large models with minimal regularization often struggle to generalize effectively. While many studies show that deep learning can achieve good generalization by measuring discrimination performance. This suggests that high-capacity models are not immune to overfitting; instead, overfitting tends to manifest as probabilistic error rather than as straightforward classification error.\n",
    "\n",
    "![](images_uq/dl_cal.png)\n",
    "\n",
    "Guo C, Pleiss G, Sun Y, Weinberger KQ. On Calibration of Modern Neural Networks 2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring functions\n",
    "\n",
    "### Brier score\n",
    "\n",
    "$$BS = \\frac{1}{N}\\sum_{n=1}^N (\\hat{p} - y)$$\n",
    "\n",
    "- $\\hat{p}$ is predicted probability\n",
    "- $y$ is the true class\n",
    "- Lower Brier Score indicates better calibration and accuracy of the uncertainty estimates.\n",
    "\n",
    "> Brier score is a measure of **BOTH** **calibration** and **accuracy**\n",
    "\n",
    "### Negative Log-Likelihood (NLL)\n",
    "\n",
    "$$\\text{NLL} = -\\frac{1}{N} \\sum_{n=1}^N y_n\\log( p(y_n)  + (1-y_n)\\log(1- p(y_n) )$$\n",
    "\n",
    "- $\\hat{p}$ is predicted probability\n",
    "- $y$ is the true class\n",
    "- Lower NLL indicates better calibration and accuracy of the uncertainty estimates.\n",
    "\n",
    "> NLL is a measure of **BOTH** **calibration** and **accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misclassification\n",
    "\n",
    "In the binary classification setting, an uncertainty threshold is employed to differentiate between positive (i.e., certain) and negative (i.e., uncertain) samples. The outcomes of this classification are then compared against the true labels of each sample, which can be categorized as either correct or incorrect. From this perspective, a confusion matrix can be constructed, distinguishing four possible cases with the following counts:\n",
    "\n",
    "- True Positive (TP): The prediction is uncertain, and the expected label and the prediction differ\n",
    "- False Negative (FP): The prediction is certain, but the expected label and the prediction differ\n",
    "- True Negative (TN): The prediction is certain, and the expected label and the prediction are identical\n",
    "- False Negative (FN): The prediction is uncertain, but the prediction and the expected label are identical\n",
    "\n",
    "![](images_uq/misclass.png)\n",
    "\n",
    "For \n",
    "\n",
    "- $L_i = \\mathcal{1} (\\hat{y} = y)$\n",
    "- $c(x)$ is uncertainty estimate\n",
    "\n",
    "![](images_uq/misclass2.png) ![](images_uq/misclass3.png)\n",
    "\n",
    "Since the optimal threshold is often unknown, we can evaluate performance using the area under the curve (AUC), which assesses all possible thresholds.\n",
    "\n",
    "- The Area Under the Curve (AUC), whether for the Receiver Operating Characteristic (ROC) or Precision-Recall (PR) curves, serves as a comprehensive summary metric for model performance across varying thresholds.\n",
    "\n",
    "### Out-of-Distribution (OOD) and Corruption Detection\n",
    "\n",
    "This methodology can also be applied to out-of-distribution (OOD) detection, where we assume that OOD data is characterized by high uncertainty. In this context, positive samples are defined as OOD, while negative samples are classified as in-distribution (ID). By leveraging uncertainty estimates, we can effectively identify samples that deviate from the expected distribution, thereby enhancing the robustness of the classification model in real-world scenarios. This approach not only improves the model's reliability but also aids in mitigating the risks associated with misclassifying OOD data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interval evaluations\n",
    "\n",
    "### Coverage probability\n",
    "\n",
    "$$P(y_l< y < y_u)$$\n",
    "\n",
    "- Coverage Probability measures the proportion of prediction intervals that successfully contain the true label y.\n",
    "- A higher coverage probability indicates a more reliable UQ method, as it suggests that the intervals are accurately capturing the true values.\n",
    "\n",
    "### Interval Width\n",
    "\n",
    "Interval Width w is calculated as: $w=y_u‚Äã‚àíy_l‚Äã$\n",
    "\n",
    "- A narrower interval indicates more precise predictions, while a wider interval suggests greater uncertainty.\n",
    "- Balance between interval width and coverage probability is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive uncertainty model\n",
    "\n",
    "As a modeller, one is mainly interested in the uncertainty that is propagated onto a prediction $\\hat{y}$ , the so-called predictive uncertainty. Within the data acquisition model, the probability distribution for a prediction $\\hat{y}$ based on some sample $x$ is given by\n",
    "\n",
    "$$p(y|x, D) = \\int p(y|x, \\theta) p(\\theta | D) d \\theta $$\n",
    "\n",
    "1. Epistemic uncertainty\n",
    "    - ‚ÄúI am not sure because I have not seen it before.‚Äù \n",
    "    - Model uncertainty\n",
    "2. Aleatoric uncertainty\n",
    "    - ‚ÄúI have experienced it before, I know what I am doing, but I think there is more than one good answer to your question, so I cannot choose just one.‚Äù\n",
    "    - Data uncertainty\n",
    "\n",
    "![](images_uq/epis.png)\n",
    "\n",
    "Yang, CI., Li, YP. Explainable uncertainty quantifications for deep learning-based molecular property prediction. J Cheminform 15, 13 (2023). https://doi.org/10.1186/s13321-023-00682-3\n",
    "\n",
    "![](images_uq/vs.png)\n",
    "\n",
    "Alex Kendall and Yarin Gal. What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision? 2017. doi: 10.48550/ARXIV.1703.04977. url: https://arxiv.org/abs/1703.04977.\n",
    "\n",
    "## Epistemic uncertainty\n",
    "\n",
    "- ‚ÄúI am not sure because I have not seen it before.‚Äù \n",
    "- Model uncertainty\n",
    "\n",
    "![](images_uq/epis2.png) ![](images_uq/epis3.png)\n",
    "\n",
    "$$p(y|x, D) = \\int p(y|x, \\theta) p(\\theta | D) d \\theta $$\n",
    "\n",
    "$$p(\\theta | D) \\propto p(\\theta )p(D|\\theta)$$\n",
    "\n",
    "- There are several possible classifiers compatible with the data we have observed (we are unsure about our prediction because several models could fit the training data).\n",
    "- We wish to sample data from underexplored regions to increase our certainty in the choice of the model.\n",
    "\n",
    "**Prior Hypothesis space $p(\\theta)$**\n",
    "\n",
    "![](images_uq/epis_prior.png)\n",
    "\n",
    "Epistemic uncertainty cause by\n",
    "\n",
    "1. The size of our training set is too small, and so the variance of our estimator is too high. \n",
    "2. The training data distribution does not cover some meaningful regions in the input space; there are some underexplored areas eg Out of distribution, distribution shift.\n",
    "\n",
    "Note: Epistemic uncertainty is reducible. It stems from limitations in current knowledge and can be mitigated through data collection or model improvement.\n",
    "\n",
    "#### Example: Distribution shift\n",
    "\n",
    "![](images_uq/epis_ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aleatoric uncertainty\n",
    "\n",
    "- ‚ÄúI have experienced it before, I know what I am doing, but I think there is more than one good answer to your question, so I cannot choose just one.‚Äù\n",
    "- Data uncertainty\n",
    "\n",
    "![](images_uq/alea.png)\n",
    "\n",
    "Battleday et al. 2019 ‚ÄúImproving machine classification using human uncertainty measurements‚Äù https://openreview.net/pdf?id=rJl8BhRqF7\n",
    "\n",
    "Aleatoric uncertainty, also known as data uncertainty, arises from information loss or noise in the data itself. Unlike epistemic uncertainty, which can be reduced by improving the model or adding more data, aleatoric uncertainty is irreducible‚Äîit represents uncertainty that is intrinsic to the data and cannot be eliminated through model adjustments.\n",
    "\n",
    "![](images_uq/alea2.png)\n",
    "\n",
    "Vaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex Fang, Benjamin Recht, and Ludwig Schmidt. 2020. Evaluating machine accuracy on ImageNet. In Proceedings of the 37th International Conference on Machine Learning (ICML'20), Vol. 119. JMLR.org, Article 801, 8634‚Äì8644.\n",
    "\n",
    "In image there are many object such as screen and coffee mug. Model may learn the association between object and cannot sure whather the correct answer is.\n",
    "\n",
    "![](images_uq/alea3.png)\n",
    "\n",
    "Vaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex Fang, Benjamin Recht, and Ludwig Schmidt. 2020. Evaluating machine accuracy on ImageNet. In Proceedings of the 37th International Conference on Machine Learning (ICML'20), Vol. 119. JMLR.org, Article 801, 8634‚Äì8644.\n",
    "\n",
    "You not sure whether it is Rabbit or Duck\n",
    "\n",
    "![](images_uq/alea4.png)\n",
    "\n",
    "Battleday et al. 2019 ‚ÄúImproving machine classification using human uncertainty measurements‚Äù https://openreview.net/pdf?id=rJl8BhRqF7\n",
    "\n",
    "Missing features can introduce overlaps in two classes. When we have both features x1 and x2, points of the same label are well separated. For example, we know all the pixels in the N-digit MNIST example. We have no aleatoric uncertainty. As soon as we remove some features (e.g., a part of the image), we have overlaps between the classes.\n",
    "\n",
    "![](images_uq/alea5.png)\n",
    "\n",
    "An Y, Lam HK, Ling SH. Auto-Denoising for EEG Signals Using Generative Adversarial Network. Sensors. 2022; 22(5):1750. https://doi.org/10.3390/s22051750 \n",
    "\n",
    "EEG sensor noise.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epistemic vs Aleatoric uncertainty\n",
    "\n",
    "![](images_uq/alea_epis.png)\n",
    "\n",
    "H√ºllermeier E, Waegeman W. Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods. Mach Learn 2021;110:457‚Äì506.\n",
    "\n",
    "\n",
    "![](images_uq/alea_epis2.png)\n",
    "\n",
    "Gawlikowski J, Tassi CRN, Ali M, Lee J, Humt M, Feng J, et al. A survey of uncertainty in deep neural networks. Artif Intell Rev 2023;56:1513‚Äì89.\n",
    "\n",
    "### Source of uncertainty in practice\n",
    "\n",
    "1. the data: The occurrence of some information in the environment (e.g. a bird‚Äôs singing) and a measured observation of this information (e.g. an audio record). \n",
    "    - the variability in real world situations eg. out of distribution (Epistemic)\n",
    "    - the errors inherent to the measurement systems (Aleatoric)\n",
    "    - unknown data (Epistemic)\n",
    "2. the DNN building process: The design and training of a neural network. (Epistemic)\n",
    "    - Model architecture (Epistemic)\n",
    "    - Model training process (Epistemic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epistemic Uncertainty Quantification\n",
    "\n",
    "Epistemic uncertainty represents uncertainty due to a lack of knowledge about the optimal model. It can be quantified using these idea:\n",
    "\n",
    "1. Modeling Parameter Distributions: Estimating uncertainty in model parameters.\n",
    "2. Out-of-Distribution (OOD) Detection: Assessing if data samples are dissimilar to the training set, often using distance metrics or anomaly detection techniques.\n",
    "\n",
    "## Emsemble learning\n",
    "\n",
    "In model ensembling, we train several deterministic models on the same (or subsampled) data simultaneously. We approximate $p(\\theta | D)$ as ensemble of $M$ models.\n",
    "\n",
    "$$p(\\theta | D) \\approx \\frac{1}{M} \\sum_{m=1}^M \\delta(\\theta - \\theta^{m})$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\begin{split}\n",
    "p(y|x, D) =& \\int p(y|x, \\theta) p(\\theta | D) d \\theta \\\\\n",
    "\\approx & \\int p(y|x, \\theta) \\frac{1}{M} \\sum_{m=1}^M \\delta(\\theta - \\theta^{m}) d \\theta \\\\\n",
    "= & \\sum_{m=1}^M\\int p(y|x, \\theta) \\frac{1}{M} \\delta(\\theta - \\theta^{m}) d \\theta \\\\\n",
    "= & \\frac{1}{M}\\sum_{m=1}^M p(y|x, \\theta^{m})\n",
    "\\end{split}$$\n",
    "\n",
    "This corresponds to averaging the predictions of individual models.\n",
    "\n",
    "Pro:\n",
    "- Conceptually simple ‚Äì run the training algorithm M times and average outputs.\n",
    "- Applicable to a wide range of models ‚Äì from linear regression to ChatGPT.\n",
    "- Parallelizable ‚Äì if we have a lot of computational resources, we can train multiple models simultaneously on different cluster nodes (GPUs)\n",
    "- Performant ‚Äì ensembles are not only able to represent epistemic uncertainty but are also often more accurate.\n",
    "\n",
    "Contra:\n",
    "- Ensembles do not realize the full potential of Bayesian ML (no infinite number of models, no connectivity between the models).\n",
    "- Space and time complexities scale linearly with M . If we have a limited number of GPUs, we must wait until the previous model finishes training\n",
    "## Baysian methods\n",
    "\n",
    "### MC dropout\n",
    "\n",
    "Dropout, commonly used as a regularization technique, also serves as a form of Bayesian approximation when used during both training and testing phases. In this approach, we can interpret dropout as an approximation to a probabilistic model by drawing multiple \"sampled\" models through random network weight adjustments. This aligns with the Bayesian framework where dropout effectively **simulates an ensemble of models**.\n",
    "\n",
    "At inference time, we can apply dropout in a way similar to model ensembling by performing multiple stochastic forward passes. The predictive distribution of the output ùë¶ given input ùë• and data ùê∑ is approximated as:\n",
    "\n",
    "$$\\begin{split}\n",
    "p(y|x, D) =& \\int p(y|x, \\theta) p(\\theta | D) d \\theta \\\\\n",
    "\\approx& \\frac{1}{K}\\sum_{m=1}^K p(y|x, \\theta^{k})\n",
    "\\end{split}$$\n",
    "\n",
    "For $K$ is the number of times we run (samples) the models.\n",
    "\n",
    "\n",
    "![](images_uq/mcdrop.png)\n",
    "\n",
    "Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. 2016. url: https://arxiv.org/abs/1612.01474.\n",
    "\n",
    "These days, MC dropout is treated as a method that does not really work.\n",
    "\n",
    "### Variational inference\n",
    "\n",
    "![](images_uq/bayes.png)\n",
    "\n",
    "In variational inference, we assume that the posterior distribution of model parameters can be approximated by a simpler distribution, such as a Gaussian distribution\n",
    "\n",
    "![](images_uq/var.png)\n",
    "\n",
    "$$p(\\theta | D) \\sim \\mathcal{N}(\\theta| \\mu,\\sigma)$$\n",
    "\n",
    "then\n",
    "\n",
    "$$\\begin{split}\n",
    "p(y|x, D) =& \\int p(y|x, \\theta) p(\\theta | D) d \\theta \\\\\n",
    "\\approx & \\int p(y|x, \\theta)  \\mathcal{N}(\\theta| \\mu,\\sigma) d \\theta \\\\\n",
    "\\approx& \\frac{1}{K}\\sum_{m=1}^K p(y|x, \\theta^{k})\n",
    "\\end{split}$$\n",
    "\n",
    "Similar to what we do in MCDropout, we can sample $\\theta$ from normal distribution (approximate distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aleatoric uncertainty quantification\n",
    "\n",
    "To represent aleatoric uncertainty, we rely on two key components in our model design:\n",
    "\n",
    "1. Model Architecture: Formulate a model architecture that accommodates multiple possible outputs. We should prepare, e.g., a probabilistic output where our model outputs the parameters of this output distribution rather than a single prediction.\n",
    "\n",
    "2. Loss Function: Use a loss function that aligns the predicted output distribution with the observed data distribution. For example, in regression tasks, a negative log-likelihood loss can be used to train the model to fit the predicted distribution to the data, where higher predicted variance is encouraged for noisier data points. This approach ensures the model‚Äôs uncertainty estimates are consistent with the data variability.\n",
    "\n",
    "## Loss function\n",
    "\n",
    "Loss functions guide the model to make predictions that represent probability distributions over classes. A commonly used loss function in classification tasks is the Negative Log-Likelihood (NLL), or equivalently, cross-entropy loss:\n",
    "\n",
    "$$L = -\\sum_k y_k \\log(f_k(x))$$\n",
    "\n",
    "where $f_k(x)$ is the predicted probability for class ùëò, and $y_k$ is the true label, often represented as a one-hot encoded vector.\n",
    "\n",
    "- DNN is encouraged to predict $f_k(X)$ that correctly represents the spread $p(Y|X)$ in the training set.\n",
    "- The popular softmax + cross-entropy design is already capable of handling aleatoric uncertainty!\n",
    "\n",
    "### Calibration methods\n",
    "\n",
    "However, these predicted probabilities $f_k(x)$ are not always well-calibrated, meaning they may not accurately reflect the true likelihood of an outcome. For instance, if a model predicts a 90% probability for a class, it should be correct about 90% of the time for that class. In practice, models‚Äîespecially deep networks‚Äîoften overestimate or underestimate their confidence.\n",
    "\n",
    "Calibration aims to adjust the predicted probabilities to make them more reliable. Techniques like temperature scaling, Platt scaling, and isotonic regression adjust the model outputs to produce well-calibrated probabilities. For example:\n",
    "\n",
    "1. Temperature Scaling\n",
    "2. Platt Scaling\n",
    "3. Isotonic Regression\n",
    "\n",
    "**Temperature scaling**\n",
    "\n",
    "Temperature scaling is a simple but effective post-processing calibration technique. It involves dividing the logits (the raw output of the neural network before softmax) by a temperature parameter ùëá before applying the softmax function:\n",
    "\n",
    "$$\\text{softmax}(\\frac{z}{T})$$\n",
    "\n",
    "![](images_uq/temp.png)\n",
    "\n",
    "Guo C, Pleiss G, Sun Y, Weinberger KQ. On Calibration of Modern Neural Networks 2017.\n",
    "\n",
    "**Platt Scaling**\n",
    "\n",
    "![](images_uq/platt.png)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Platt_scaling\n",
    "\n",
    "In binary classification, Platt scaling fits a **logistic regression model** on the raw outputs (logits) to map them to calibrated probabilities. For multi-class problems, it can be extended by applying it to each class independently in a one-vs-rest manner.\n",
    "\n",
    "**Isotonic Regression**\n",
    "\n",
    "![](images_uq/iso.png)\n",
    "\n",
    "https://en.wikipedia.org/wiki/Isotonic_regression\n",
    "\n",
    "Isotonic regression finds a piecewise constant function that best fits the validation data. Unlike temperature and Platt scaling, it does not assume a specific functional form (non-parametric). This makes isotonic regression very flexible, as it allows for highly nonlinear mappings.\n",
    "\n",
    "## Mixture of Gaussians\n",
    "\n",
    "Example:\n",
    "\n",
    "In the training set, we will see many cases of mixed supervision. There will be many cases where a vehicle will take turns (1), (2), or (3), coming from the same direction. Therefore, we need something better than a single distribution to approximate the true multimodal conditional distribution. We have to accommodate all the complexity that can happen in the future.\n",
    "\n",
    "![](images_uq/mog3.png)\n",
    "\n",
    "A Mixture of Gaussians (MoG) is a probabilistic model that assumes data points are generated from a mixture of multiple Gaussian distributions, each with its own mean and variance. This model is particularly useful in cases where the data has complex, multimodal distributions that a single Gaussian cannot capture effectively.\n",
    "\n",
    "![](images_uq/mog.png)\n",
    "\n",
    "![](images_uq/mog2.png)\n",
    "\n",
    "## Test‚Äëtime augmentation\n",
    "\n",
    "Test-Time Augmentation (TTA) is a practical technique used in uncertainty quantification to improve predictions and measure model confidence by leveraging data transformations at **inference time**. Instead of predicting on a single test sample, TTA generates multiple augmented versions of the test data through transformations commonly applied during training, like cropping, flipping, rotating, or adjusting brightness. This allows the model to make predictions across slightly varied perspectives of the same input, yielding a distribution of predictions rather than a single output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application in Medicine\n",
    "\n",
    "![](images_uq/app.png)\n",
    "\n",
    "Zhang X, Sisniega A, Zbijewski WB, Lee J, Jones CK, Wu P, et al. Combining physics‚Äêbased models with deep learning image synthesis and uncertainty in intraoperative cone‚Äêbeam CT of the brain. Medical Physics 2023;50:2607‚Äì24.\n",
    "\n",
    "\n",
    "![](images_uq/ex.png)\n",
    "\n",
    "Pereira T, Cardoso S, Guerreiro M, Mendon√ßa A, Madeira SC. Targeting the uncertainty of predictions at patient-level using an ensemble of classifiers coupled with calibration methods, Venn-ABERS, and Conformal Predictors: A case study in AD. Journal of Biomedical Informatics 2020;101:103350.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
