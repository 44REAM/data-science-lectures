{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- Trustworthy Machine Learning Lecture at the University of Tübingen. \n",
    "Winter Semester 2023/2024. https://www.youtube.com/playlist?list=PL05umP7R6ij0FDHxle4CQLkzOfN-PAt7b ***\n",
    "- Mucsányi B, Kirchhof M, Nguyen E, Rubinstein A, Oh SJ. Trustworthy Machine Learning 2023. ***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustworthy Machine learning 1: OOD generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "> **Are we done with ML? Is there anything else to research?**\n",
    "\n",
    "## Limitation\n",
    "\n",
    "- Dataset: data collection is quite expensive\n",
    "    - Collect yourself\n",
    "    - Use Public data\n",
    "    - Buy Data\n",
    "- Resource\n",
    "    - Hardware\n",
    "    - Human Resource\n",
    "- It dose not work\n",
    "    - Given the same distribution of data, ML models can generalized well\n",
    "    - However, if new situation (not the same distribution). The model will not work.\n",
    "- ML model is not **trustworthy**, especially if the task is related to your life.\n",
    "    - Imagine 10 AI doctors that you have cancer, will you start treatment without consulting human doctor?\n",
    "    - Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Topics in Trustworthy AI\n",
    "\n",
    "1. Out of distribution (ODD) generalization: Can we train the model to work well beyond the training distribution?\n",
    "2. Explainability: Can we make a model explain itself?\n",
    "3. Uncertainty: Can we make a model know when it dose not know?\n",
    "4. Evaluation: How to quantify trustworthiness?\n",
    "5. Fairness: Demographic disparity and use of sensitive attributes.\n",
    "6. Privacy and security: How to keep data safe?\n",
    "7. Abuse of AI tools: Deepfake\n",
    "8. Environmental concern: GPU consumes a lot of energy\n",
    "9. Governance and Ethic: Regulation around the use of AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving \"What\" is not enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/car.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The \"how\" part of the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/car1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You dont know whether the model learn \"Road\" or \"Car\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/car2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifted focus in ML: \"how\" problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic dataset: extracted foreground and randomly change the background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/how_Cv.png)\n",
    "\n",
    "J. Djolonga et al., “On robustness and transferability of convolutional neural networks,” 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16453–16463, Jun. 2021. doi:10.1109/cvpr46437.2021.01619 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/how_LLM.png)\n",
    "\n",
    "Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2022, January 28). Chain-of-Thought prompting elicits reasoning in large language models. arXiv.org. https://arxiv.org/abs/2201.11903"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Generalization?\n",
    "\n",
    "![](images/gen.png)\n",
    "\n",
    "![](images/gen2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why we need OOD Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=sbSDsbDQjSU\n",
    "\n",
    "![](images/reason_ood.png)\n",
    "\n",
    "![](images/reason_ood2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once we “solve” the OOD generalisation…\n",
    "\n",
    "- Your model will work well even under new situations.\n",
    "- The MLOps will not be needed in the current scale.\n",
    "- Small businesses will be able to adopt ML more easily.\n",
    "- ML can be extended to more risky applications, eg healthcare, transportation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOD generalisation is required when development environment does not fully reflect the deployment environment.\n",
    "\n",
    "![](images/ood.png)\n",
    "\n",
    "#### Development stage and Deployment stage\n",
    "\n",
    "**Development stage**\n",
    "\n",
    "- Stage for preparing ML model for deployment.\n",
    "- Development resources: Labelled samples (X,Y)\n",
    "\n",
    "**Deployment stage**\n",
    "- Actual users use the ML model prepared in the dev stage.\n",
    "- Common to keep track of performance measurement in deployment stage.\n",
    "- Deployment environment: Unlabelled samples X \n",
    "\n",
    "**In Practice**\n",
    "\n",
    "The boundary between development and deployment stages can often blur.\n",
    "Deployed models may be continuously updated by leveraging feedback or data from the deployment stage to refine the model iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "- Simulate deployment conditions to assess model readiness and robustness.\n",
    "- Enable effective model selection\n",
    "\n",
    "#### Evaluation components\n",
    "\n",
    "- Datasets\n",
    "- Evaluation Metrics: Metrics that capture performance in diverse settings.\n",
    "- Dataset Usage Rules: Guidelines for train-test splits and OOD simulation strategies.\n",
    "- Additional Information Sources: Rules for leveraging OOD simulations, pretrained models, or other external data sources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulating a real world ML setting via data splits\n",
    "\n",
    "![](images/split.png)\n",
    "\n",
    "- Train: Dev samples where the model parameters are updated.\n",
    "- Val: Dev samples on which hyperparameters and discrete design choices are made.\n",
    "- Test: Testing is a lab setup designed to mimic the deployment scenario closely.\n",
    "\n",
    "**Practice point of view on Testing**: \n",
    "- This is different from deployment and still a part of development.\n",
    "- As soon as we have labeled samples from deployment (and we make any design choices based on these or just test our model), we are using information from the deployment setup in dev. We cannot talk about true (domain or task) generalization anymore. The deployment scenario should stay fictitious and unobserved in such settings. \n",
    "\n",
    "**Academia point of view on Testing**\n",
    "- The test set and the action of testing is treated as a part of the deployment.\n",
    "\n",
    "ML becomes arbitrarily easier if information from deployment stage is available during development.\n",
    "\n",
    "- Easy: Unlimited number of samples from the deployment distribution (ID generalisation).\n",
    "- Medium: Just a few samples from the deployment data distribution.\n",
    "- Hard: Broad language description of the type of data (e.g. city views).\n",
    "- Very hard: No information at all.\n",
    "\n",
    "> As such, (train+val) and test must be strictly separated.\n",
    "\n",
    "![](images/split2.png)\n",
    "\n",
    "#### Common knowledge\n",
    "\n",
    "- Train (Trainable parameters)\n",
    "    - (Usually) Used for tuning parameters through gradient-based optimisers.\n",
    "    - Optimisation objective is to achieve a good fit to the training data.\n",
    "- Val (Hyperparameters)\n",
    "    - Used for tuning a few hyperparameters and design choices through optimisation, e.g. grid / random / bayesian optimiser.\n",
    "    - Achieve good generalisation on the “Val” split after training on “Train”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Evaluation paradox\n",
    "\n",
    "\n",
    "![](images/paradox.png)\n",
    "\n",
    "For clean experiment, deployment data must be hidden from the developers.\n",
    "But without deployment data, you can never confirm how well you’re addressing the problem, especially for OOD generalisation.\n",
    "\n",
    "\n",
    "![](images/paradox2.png)\n",
    "\n",
    "As soon as you evaluate on deployment data, however small the portion is, you leak the information to the development stage, possibly distorting the performance metric.\n",
    "\n",
    "Once any information from deployment data leaks, corresponding deployment data are now part of development environment by definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task, Domain and Cue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "![](images/task.png)\n",
    "\n",
    "ref: https://www.researchgate.net/figure/Computer-vision-tasks-Adapted-from_fig5_346091812\n",
    "\n",
    "![](images/task2.png)\n",
    "\n",
    "ref: https://subscription.packtpub.com/book/application-development/9781788478311/1/ch01lvl1sec11/tasks-of-natural-language-processing\n",
    "\n",
    "![](images/task3.png)\n",
    "\n",
    "ref: https://www.researchgate.net/figure/The-color-map-of-20-pre-specified-object-classes-of-Pascal-VOC-2010-8_fig1_281717283\n",
    "\n",
    "Even within “classification”, there exists a diversity of tasks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cue\n",
    "\n",
    "> Cues = features = attributes\n",
    "\n",
    "Cues, features, and attributes all refer to the factors of variations in the data sample.\n",
    "\n",
    "![](images/cue.png)\n",
    "\n",
    "- Cue/Feature: Any factor of variation within data. Cues are inherent to the data.\n",
    "\n",
    "- Task: The factor of variation that matters for you. The factor that you need to recognise at deployment. Tasks are not inherent to the data; it needs to be defined by a human.\n",
    "\n",
    "Example: \n",
    "\n",
    "- Task: Shape classification\n",
    "- Cue: Cue 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domains\n",
    "\n",
    "Domains = Environments \n",
    "\n",
    "![](images/domain.png)\n",
    "\n",
    "#### Example of OOD generalization problem\n",
    "\n",
    "![](images/domain2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Settings related/similar to OOD Generalization\n",
    "\n",
    "**Different Domain/Distribution Settings**\n",
    "\n",
    "- In-Distribution (ID) Generalization\n",
    "- Out-of-Distribution (OOD) Generalization\n",
    "- Domain adaptation\n",
    "- Domain generalisation\n",
    "- Test-time training\n",
    "- Continual learning\n",
    "\n",
    "**Different Task Settings**\n",
    "\n",
    "- Transfer learning\n",
    "- K-shot learning\n",
    "- Meta-Learning\n",
    "- Continual learning\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ID Generalization\n",
    "![](images/id_gen.png)\n",
    "\n",
    "- In-Distribution (ID) Generalization: The model performs well on data that follows the same distribution as the training data.\n",
    "- Example: A model trained on handwritten digits (e.g., MNIST dataset) and evaluated on similar handwritten digits from the same population.\n",
    "\n",
    "#### OOD Generalization\n",
    "\n",
    "![](images/ood_gen.png)\n",
    "\n",
    "![](images/gen.png)\n",
    "\n",
    "- In cross-domain generalization, the deployment environment contains completely unseen cues in dev. \n",
    "- In cross-bias generalization, deployment contains unseen compositions of seen cues in dev. \n",
    "- Adversarial generalization considers a (real/hypothetical) adversary in deployment who tries to choose the worst-case domain.\n",
    "\n",
    "#### Domain Adaptation\n",
    "\n",
    "![](images/da.png)\n",
    "\n",
    "- Access to Target Domain Samples: During development, we have some labeled or unlabeled samples from the deployment environment.\n",
    "- Feature Alignment: Align feature distributions with target domain statistics to bridge the gap between source and target domains.\n",
    "\n",
    "![](images/da2.png)\n",
    "\n",
    "**Idea**\n",
    "\n",
    "- Apply penalties, such as an L2 penalty on differences between domain statistics (e.g., sample mean and variance).\n",
    "- Alternatively, use distance metrics as a penalty to measure the similarity between distributions.\n",
    "\n",
    "#### Domain Generalization\n",
    "\n",
    "![](images/dg.png)\n",
    "\n",
    "- During the dev stage, we have access to labeled samples from multiple domains. We also know the domain label for every sample.\n",
    "- Aim is to “unlearn” domain-related characteristics in your representation by performing domain statistic matching similarly to domain adaptation.\n",
    "\n",
    "\n",
    "#### Test-Time Training\n",
    "\n",
    "![](images/ttt.png)\n",
    "\n",
    "- After training your model, you keep updating your model according to the unlabelled samples from the deployment environment.\n",
    "- Dev continues even into the deployment in the sense that your model keeps getting updated.\n",
    "\n",
    "#### Domain-Incremental Continual Learning\n",
    "\n",
    "![](images/cl.png)\n",
    "\n",
    "- Domains are added over time.\n",
    "\n",
    "#### Transfer learning\n",
    "\n",
    "![](images/tl.png)\n",
    "\n",
    "- Use pretrained model (on upstream task) & **fine tune** it to the target (downstream) task. → Improve performance for target task.\n",
    "- Transfer learning can involve transferring knowledge across *different domains* as well as different tasks. Domain adaptation is a subcategory of transfer learning.\n",
    "\n",
    "**In practice**\n",
    "\n",
    "- Computer vision:\n",
    "    - Timm models: https://github.com/huggingface/pytorch-image-models, https://huggingface.co/timm\n",
    "    - Pytorch models: https://pytorch.org/vision/stable/models.html\n",
    "- Natural langauge processing\n",
    "    - Huggingface models: https://github.com/huggingface/transformers, https://huggingface.co/docs/transformers\n",
    "\n",
    "**Varying degrees of tuning for the downstream**\n",
    "\n",
    "- Fine-tuning\n",
    "    - Further fit all trainable parameters for the downstream task.\n",
    "    - LoRA for lightweight tuning. \n",
    "- Partial tuning\n",
    "    - Last-layer tuning \n",
    "- No tuning\n",
    "    - K-nearest neighbour classifier (Distance comparison)\n",
    "\n",
    "\n",
    "\n",
    "#### K-Shot Learning\n",
    "\n",
    "![](images/k_shot.png)\n",
    "\n",
    "- We learn to fit our model to the deployment task using a large number of task 1 samples and a few number of data (K × number of class) from task 2 samples.\n",
    "- Key Goal: Adapt to a new task with minimal examples, often by transferring knowledge from a larger dataset or through pre-trained models.\n",
    "\n",
    "- **Zero-shot learning**: The model encounters a completely new task during deployment, one it has not seen in development, and must rely on prior knowledge or task descriptions to perform.\n",
    "\n",
    "    - Large Language Models (LLMs): These models use zero-shot prompting, where they can perform tasks based on natural language descriptions without explicit training on those tasks. For instance, a chatbot can answer trivia questions it hasn't been trained on by leveraging general knowledge encoded during training.\n",
    "    - CLIP: CLIP can recognize images based on textual labels it has not explicitly seen during training, enabling zero-shot recognition by connecting visual and textual representations.\n",
    "\n",
    "**Challenges in True Zero-Shot Evaluation**\n",
    "\n",
    "- Benchmark Limitations: It’s difficult to guarantee that benchmarks for zero-shot tasks are genuinely new for LLMs. Due to the vast scope of data in their training, we cannot be certain a model hasn’t seen a similar task before.\n",
    "\n",
    "#### Meta-learning\n",
    "![](images/meta.png)\n",
    "\n",
    "- Meta-learning, or \"learning to learn,\" is an approach where the model is trained to quickly adapt to new tasks.\n",
    "- It focuses on enabling the model to learn more efficiently with limited data, making it highly suitable for few-shot learning tasks.\n",
    "- Key Goal: Teach the model to become better at learning, by exposing it to many tasks so it can discover general strategies for quick adaptation.\n",
    "\n",
    "#### Task-Incremental Continual Learning\n",
    "\n",
    "![](images/ticl.png)\n",
    "\n",
    "- Another type of continual learning (CL) is task-incremental learning.\n",
    "- Tasks are added over time. You label a few samples over time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML settings in the era of foundational models\n",
    "\n",
    "\n",
    "![](images/foundation.png)\n",
    "\n",
    "- Training set is massive (billions-trillions of tokens).\n",
    "- Deployment environment is already very likely to be part of development environment.\n",
    "- Is this a problem? \n",
    "    - In practice, not really - one could see LLM as a large, efficient, and untrustworthy DB.\n",
    "    - In academic studies & benchmarking, perhaps it is a problem - unfair comparisons.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "- Development resources\n",
    "    - Multiple image datasets D1, D2, D3, …, Dn with the same task (classification with the same set of labels Y). Each dataset consists of IID samples from distributions P1, P2, …, Pn, but each dataset follows a different distribution: Pi ≠ Pj for all i ≠ j. Every sample is labelled in each dataset.\n",
    "    - For each image, you know which dataset it belongs to.\n",
    "    - You have collected a few unlabelled samples Dn+1 from the deployment environment Pn+1.\n",
    "- Deployment environment\n",
    "    - The stream of inputs follow the distribution Pn+1. This distribution is different from those of the training datasets: Pi ≠ Pn+1 for all i=1,...,n.\n",
    "- Example real-world scenario for this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/dg.png)\n",
    "\n",
    "- According to the “domain generalisation” definition above, information about Domain 4 must not be available during development.\n",
    "- When it is available, you can’t call it a “domain generalisation” anymore.\n",
    "\n",
    "#### Leakage examples\n",
    "\n",
    "- Some hyperparameters are chosen based on labeled samples from domain 4. In a sense, our dev set is partly taken from domain 4.\n",
    "- The model is trained on labeled samples from domains 1-3 and unlabeled samples from domain 4, we are performing *domain adaptation*, not domain generalization.\n",
    "- Information Leakage from Domain Generalization Evaluation\n",
    "    - It’s common to evaluate the model on labeled samples from Domain 4 to measure performance in a new deployment environment. However, as soon as we use Domain 4 for evaluation, we are no longer strictly in a domain generalization setting, since the model has interacted with the target domain.\n",
    "    - Evaluating domain generalization models on benchmarks is paradoxical, as it often involves repeated observations of the target domain. While researchers continue to evaluate methods on domain generalization benchmarks, this practice technically introduces leakage but is necessary for measuring progress and effectiveness.\n",
    "\n",
    "\n",
    "#### Solutions to Information Leakage\n",
    "\n",
    "- Select hyperparameters within development resources.\n",
    "- Use the test set once per project. By using a specific test set multiple times, we can always overfit to it. If our goal is to go towards a distribution outside dev, then evaluating on the test set multiple times can be harmful.\n",
    "- Update benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOD Generalization\n",
    "\n",
    "- Cross-domain generalization: the deployment environment contains completely unseen cues in dev. \n",
    "- Cross-bias generalization: deployment contains unseen compositions of seen cues in dev. \n",
    "- Adversarial generalization: considers a (real/hypothetical) adversary in deployment who tries to choose the worst-case domain.\n",
    "\n",
    "#### Cross-domain generalisation benchmarks\n",
    "\n",
    "![](images/dg_bench.png)\n",
    "\n",
    "- The PACS Dataset: The PACS dataset considers four domains: Photos, Art Paintings, Cartoons, and Sketches: https://arxiv.org/abs/1710.03077\n",
    "- DomainBed: Combination of some popular DG benchmarks\n",
    "- The Wilds Benchmark: https://proceedings.mlr.press/v139/koh21a.html\n",
    "- ImageNet-C: Simulated corruptions under deployment scenario: https://github.com/hendrycks/robustness\n",
    "\n",
    "#### What makes CDG difficult?\n",
    "\n",
    "![](images/dif.png)\n",
    "\n",
    "- Ill-defined behaviour on novel domains. \n",
    "    - The model does not know what to do in regions without any training data.\n",
    "    - This problem can be addressed through uncertainty quantification (eg. “know when it does not know”).\n",
    "- Wrong choice of cues\n",
    "    - Spurious correlation: A spurious correlation is the co-occurrence of some cues, features, or labels, which happens in the development stage but not in the deployment stage. (All cue, color, shape achieving 100% acc in development stage)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cue selection problem\n",
    "\n",
    "### Cross-bias generalisation\n",
    "\n",
    "- Bias cue is a cue that is not integral or causal for the task, but is nonetheless (spuriously) correlated with the task.\n",
    "- Solution is to select the right cue (feature).\n",
    "    - A model under the vanilla OOD generalisation setting lacks the information to generalise well for an arbitrary deployment task.\n",
    "    - To select the right feature for the task, more information is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut bias\n",
    "\n",
    "ML model chooses easier cue for achieving 100% accuracy under the underspecified setting.\n",
    "\n",
    "E.g. according to Scimeca et al. 2022, Color > Shape in the order of preference. \n",
    "\n",
    "> Shortcut bias: Models’ inborn preference for “simpler” cues.\n",
    "\n",
    "ref:\n",
    "- Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective. ICLR 2022.\n",
    "- Underspecification Presents Challenges for Credibility in Modern Machine Learning. JMLR 2021.\n",
    "- Shortcut Learning in Deep Neural Networks. Nature Machine Intelligence 2020.\n",
    "\n",
    "> This help deep learning generalizes because the parameter-function map is biased towards simple functions. (Valle-Perez et al. 2019)\n",
    "\n",
    "ref: Deep learning generalizes because the parameter-function map is biased towards simple functions. ICLR 2019.\n",
    "\n",
    "#### Shortcut bias Problems\n",
    "\n",
    "- For ID generalisation tasks, simple cues are often sufficient for generalisation.\n",
    "- For OOD generalisation tasks, simple cues may not work anymore.\n",
    "- For fairness, simple cues may not be ethical to use.\n",
    "\n",
    "#### Context bias example\n",
    "\n",
    "\n",
    "![](images/bias.png)\n",
    "\n",
    "ref: Not Using the Car to See the Sidewalk — Quantifying and Controlling the Effects of Context in Classification and Segmentation. CVPR 2019.\n",
    "\n",
    "The task cues are the foreground objects, but a classifier focusing on the background context bias cues can achieve high accuracy when the background is highly correlated with the foreground.\n",
    "\n",
    "#### Texture bias example\n",
    "\n",
    "![](images/bias2.png)\n",
    "\n",
    "ref: ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. ICLR 2019.\n",
    "\n",
    "\n",
    "Training a cat/dog classifier on a diagonal dataset, where the texture and shape are highly correlated. At test time, we want to predict cats when changing their texture (e.g., to greyscale, silhouette, edges, or to a marginally different texture).\n",
    "\n",
    "- When only the true texture of the original object (cat) is presented, models stay perfectly accurate while humans make more mistakes (90% accuracy).\n",
    "- Networks are prone to be biased towards textures because it is much easier to learn. If the task is ‘shape’, such networks will generalize poorly to no/different textures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cue/Feature selection problem in fairness\n",
    "\n",
    "\n",
    "**What is Fairness?**\n",
    "\n",
    "In the context of equality of opportunity, fairness can be seen as a form of individual fairness: people who are similar with respect to a task should be treated similarly. Decision-makers (or models) should avoid differential treatment based on attributes like race, gender, or other characteristics, assuming these attributes are irrelevant to the task.\n",
    "\n",
    "**Fairness and Feature Selection**\n",
    "\n",
    "This is where the feature selection problem comes in:\n",
    "\n",
    "- Task Cue: Features that are directly relevant and causally linked to the task.\n",
    "- Bias Cue: Features that are correlated with the task but are not causally relevant (e.g., demographic attributes that may introduce bias).\n",
    "\n",
    "#### Example\n",
    "\n",
    "**ML-based credit evaluation system.**\n",
    "\n",
    "- Task: Predict possible future defaults for each individual.\n",
    "\n",
    "- Task cues: Size of the loans, history of repayment, income level, age, etc.\n",
    "\n",
    "- Bias cues (or sensitive attributes): Disability, gender, ethnicity, religion, …\n",
    "\n",
    "When an ML system learns to use bias cues to predict credit risks, the model is not fair. The ML system requires further guidance to not use the sensitive cues\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: Feature selection with extra information\n",
    "\n",
    "\n",
    "![](images/cue_problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1: Simple data balancing\n",
    "\n",
    "![](images/1_weight.png)\n",
    "\n",
    "\n",
    "\n",
    "- A bit of unbiased samples in dev resources (e.g. α=1%) and we know which sample are bias/unbias (bias label avaliable).\n",
    "- When trained, you may give more weight to unbiased sample in loss function\n",
    "\n",
    "ref: Idrissi, B. Y., Arjovsky, M., Pezeshki, M., & Lopez-Paz, D. (2021, October 27). Simple data balancing achieves competitive worst-group-accuracy. arXiv.org. https://arxiv.org/abs/2110.14503\n",
    "\n",
    "\n",
    "### Scenario 1: Domain-adversarial neural network\n",
    "\n",
    "![](images/dann.png)\n",
    "\n",
    "ref: Domain-adversarial training of neural networks. JMLR 2016.\n",
    "\n",
    "\n",
    "The idea of the method is to add an additional head to the model (magenta in the image) that would predict bias labels (named domain labels in the paper) and adversarially train a feature extractor (green in the image) such that the features it extracts are maximally noninformative for the additional head to predict bias labels but still informative for the original head (blue in the image) to solve the main task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2\n",
    "\n",
    "**Problem Setup**\n",
    "\n",
    "- Abundant biased samples, few unbiased samples.\n",
    "- No bias label → Don’t know which samples are biased.\n",
    "\n",
    "To solve scenario 2, we need some assumption.\n",
    "\n",
    "We previously know a shortcut bias that simple cue may selected. Therefore, we may assume that:\n",
    "\n",
    "- Bias is the first cue that is learned by a generic model.\n",
    "- Bias is the cue that is learned by a model of certain limited capacity\n",
    "    - small model\n",
    "    - CNN with smaller receptive field\n",
    "    - Transformer with shallow depth\n",
    "    - The model is trained for a small number of epochs\n",
    "\n",
    "![](images/2.png)\n",
    "\n",
    "**“Be Different” Supervision:**\n",
    "\n",
    "“Be different” supervision is a type of regularization that forces the final model to be different from the intentionally biased model.\n",
    "\n",
    "### Scenario 2: Learning from Failure\n",
    "\n",
    "![](images/2_lff.png)\n",
    "\n",
    "ref: Learning from Failure: Training Debiased Classifier from Biased Classifier. NeurIPS 2020.\n",
    "\n",
    "\n",
    "![](images/2_lff_eq.png)\n",
    "\n",
    "- Samples with high W(x) (Relative difficulty is high): Ones that the biased model cannot handle well.\n",
    "- Assume: Samples with high W(x) are unbiased ones. \n",
    "- Your final model is trained with the sample-wise weight W(x) on the loss.\n",
    "\n",
    "### Scenario 2: ReBias: Representational regularization\n",
    "\n",
    "**Concept**: Create intentionally biased CNNs by reducing receptive fields, limiting them to local (e.g., texture-based) cues.\n",
    "\n",
    "![](images/bias3.png) ![](images/bias4.png)\n",
    "\n",
    "- Large Receptive Fields capture both local and global features.\n",
    "- Small Receptive Fields restrict capture to local features only.\n",
    "\n",
    "![](images/3_rebias.png)\n",
    "\n",
    "ref: Learning De-biased Representations with Biased Representations. ICML 2020\n",
    "\n",
    "\n",
    "Using a similar “Be Different” approach, ReBias enforces statistical independence rather than sample weighting, measured with **HSIC (Hilbert-Schmidt Independence Criterion)**.\n",
    "\n",
    "![](images/3_rebias2.png)\n",
    "\n",
    "The property of this function is if $U$ and $V$ are independent, the HSIC is zero.\n",
    "\n",
    "\n",
    "\n",
    "Suppose we have two models, our final model $f$ and the intentionally biased model $g$, we want to enforce statistical independence $f (X) ⊥⊥ g(X)$ to ensure that the model $f$ we find is not equivalent to some other network $g$ with a small receptive field.\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "**Intentionally biased model**\n",
    "\n",
    "Intentionally Biased Model: Train a model constrained to capture local cues.\n",
    "\n",
    "![](images/3_rebias3.png)\n",
    "\n",
    "**Final model**\n",
    "\n",
    "Final Model: Train a more comprehensive model with loss minimization that reduces similarity to the biased model g.\n",
    "\n",
    "![](images/3_rebias4.png)\n",
    "\n",
    "During training, we update f once, then update g for a fixed f n times (n = 1 in the official implementation). There also many other options, eg., training f and g together on the same loss value.\n",
    "\n",
    "![](images/3_rebias5.png)\n",
    "\n",
    "During the optimization procedure, g tries to catch up to f (solve the task and maximize dependence). In turn, f tries to be different (run away) from g (solve the task and minimize dependence). Eventually, after doing this for a few iterations, f finally escapes the set of models G. Thus, no function in G can represent f anymore (due to the architectural constraint), and f cannot leverage the simple cue that g uses. Now, e.g., f looks at global shapes instead of texture: f becomes debiased.\n",
    "\n",
    "#### Evaluation on Action recognition\n",
    "\n",
    "![](images/3_rebias6.png)\n",
    "\n",
    "- Pure actions without background\n",
    "\n",
    "\n",
    "![](images/3_rebias7.png)\n",
    "\n",
    "Train 3D CNN as final model and 2DCNN as intentionally bias model.\n",
    "\n",
    "![](images/3_rebias8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 3\n",
    "\n",
    "**Problem Setup**\n",
    "\n",
    "In this Scenario, all samples are bias (all cue correlated with task).\n",
    "\n",
    "![](images/3.png)\n",
    "\n",
    "In such a case, we can train multiple models with diverse OOD behaviors, i.e., that have substantially different decision boundaries in the input space. We select the best-performing model on the test dataset (which is usually very small in size), e.g., based on accuracy.\n",
    "\n",
    "![](images/3_2.png)\n",
    "\n",
    "![](images/3_3.png)\n",
    "\n",
    "**Idea**\n",
    "\n",
    "1. Train an ensemble of models with some diversity regularization\n",
    "2. At test time, use a few labelled samples or human inspection to select the appropriate model that generalises well.\n",
    "\n",
    "The intuition behind this method is that diverse ensemble training can be achieved by enforcing “independence” between models through the orthogonality of input gradients.\n",
    "\n",
    "\n",
    "![](images/3_4.png)\n",
    "\n",
    "One way to achieve this is to add an orthogonality constraint to the loss.14 Such a constraint can be represented as the squared cosine similarity of the input gradients for the same input\n",
    "\n",
    "***Intuition***\n",
    "\n",
    "Suppose that we have two models, m1 and m2, and two different regions of the image: background and foreground. If m2 is looking at the background, there is a significant focus on the background parts in the input gradient. We want the input gradient of m1 to be orthogonal to that of m2, as that will result in m1 focusing more on the foreground."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
